\hypertarget{classflex_gradient_operator}{}\section{flex\+Gradient\+Operator$<$ T $>$ Class Template Reference}
\label{classflex_gradient_operator}\index{flex\+Gradient\+Operator$<$ T $>$@{flex\+Gradient\+Operator$<$ T $>$}}


represents a gradient operator  




{\ttfamily \#include $<$flex\+Gradient\+Operator.\+h$>$}

Inheritance diagram for flex\+Gradient\+Operator$<$ T $>$\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classflex_gradient_operator}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classflex_gradient_operator_aa83ae15b09ba2195951d8df803f08705}{flex\+Gradient\+Operator} (std\+::vector$<$ int $>$ A\+Input\+Dimension, int a\+Grad\+Direction, \hyperlink{tools_8h_a40b3c158323c8bcb80e2095f3473213c}{gradient\+Type} a\+Type, bool a\+Minus)
\begin{DoxyCompactList}\small\item\em initializes the gradient operator \end{DoxyCompactList}\item 
\hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$ $\ast$ \hyperlink{classflex_gradient_operator_a4b1480051ac7763da809c509685316d2}{copy} ()
\begin{DoxyCompactList}\small\item\em copies the linear operator \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classflex_gradient_operator_a1e71bbd56e480f5444ff6e32b0002040}\label{classflex_gradient_operator_a1e71bbd56e480f5444ff6e32b0002040}} 
void {\bfseries update\+Value} (T $\ast$ptr, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s, T value)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_ad468dc07bc6e302bf8931d45ba8efaa6}\label{classflex_gradient_operator_ad468dc07bc6e302bf8931d45ba8efaa6}} 
void {\bfseries dxp3d} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a7435c2408c7be34206a4662da37fe058}\label{classflex_gradient_operator_a7435c2408c7be34206a4662da37fe058}} 
void {\bfseries dyp3d} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a5eefd707f1f71f20c2abbe7160d660b3}\label{classflex_gradient_operator_a5eefd707f1f71f20c2abbe7160d660b3}} 
void {\bfseries dzp3d} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a6de07d9888e203f7eaa0f630f3d57c18}\label{classflex_gradient_operator_a6de07d9888e203f7eaa0f630f3d57c18}} 
void {\bfseries dxp3d\+Transposed} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a0c5efa1ceda6ee9e0ad626150557fc4b}\label{classflex_gradient_operator_a0c5efa1ceda6ee9e0ad626150557fc4b}} 
void {\bfseries dyp3d\+Transposed} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a1a00cc0370e270af955cac8174d30285}\label{classflex_gradient_operator_a1a00cc0370e270af955cac8174d30285}} 
void {\bfseries dzp3d\+Transposed} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a89682c6d26bb9a8aa7ea1fc57a734009}\label{classflex_gradient_operator_a89682c6d26bb9a8aa7ea1fc57a734009}} 
void {\bfseries dxp2d} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a3e2d15e37779ce04a9f082dfd127f545}\label{classflex_gradient_operator_a3e2d15e37779ce04a9f082dfd127f545}} 
void {\bfseries dyp2d} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_ab2266e66d3afe4d6c3752e145d07444c}\label{classflex_gradient_operator_ab2266e66d3afe4d6c3752e145d07444c}} 
void {\bfseries dxp2d\+Transposed} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a202c728fbd7909e2926d78f35d2dedfc}\label{classflex_gradient_operator_a202c728fbd7909e2926d78f35d2dedfc}} 
void {\bfseries dyp2d\+Transposed} (const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_ae006fcafaae94e9b905fa4dba5296e80}\label{classflex_gradient_operator_ae006fcafaae94e9b905fa4dba5296e80}} 
void {\bfseries do\+Times\+C\+PU} (bool transposed, const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a45cd508b73224f5339bc467e55876b80}\label{classflex_gradient_operator_a45cd508b73224f5339bc467e55876b80}} 
void {\bfseries do\+Times\+C\+U\+DA} (bool transposed, const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a75d702d6d1cc0fce5d5c45c2294aba59}\label{classflex_gradient_operator_a75d702d6d1cc0fce5d5c45c2294aba59}} 
void {\bfseries do\+Times} (bool transposed, const Tdata \&input, Tdata \&output, \hyperlink{tools_8h_ab8be8fa992a31c15058261e81ef8ba9d}{my\+Sign} s)
\item 
void \hyperlink{classflex_gradient_operator_a1b6c9b788e6d5a62ba008811f287f8e5}{times\+Plus} (bool transposed, const Tdata \&input, Tdata \&output)
\begin{DoxyCompactList}\small\item\em applies linear operator on vector and adds its result to y \end{DoxyCompactList}\item 
void \hyperlink{classflex_gradient_operator_a287f5efd41aa14ee61aee87dfed08b88}{times\+Minus} (bool transposed, const Tdata \&input, Tdata \&output)
\begin{DoxyCompactList}\small\item\em applies linear operator on vector and substracts its result from y \end{DoxyCompactList}\item 
void \hyperlink{classflex_gradient_operator_aae5e807f99c3634c52b79f08d72fa7a2}{times} (bool transposed, const Tdata \&input, Tdata \&output)
\begin{DoxyCompactList}\small\item\em applies linear operator on vector \end{DoxyCompactList}\item 
T \hyperlink{classflex_gradient_operator_a6acb61ea8abf404d63be4574976391bb}{get\+Max\+Row\+Sum\+Abs} (bool transposed)
\begin{DoxyCompactList}\small\item\em returns the maximum sum of absolute values per row used for preconditioning \end{DoxyCompactList}\item 
std\+::vector$<$ T $>$ \hyperlink{classflex_gradient_operator_a04950a1e57f7587b95824bfd82b35738}{get\+Abs\+Row\+Sum} (bool transposed)
\begin{DoxyCompactList}\small\item\em returns a vector of sum of absolute values per row used for preconditioning \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classflex_gradient_operator_add0b4fb2eed25bc6386abd8219e516ff}\label{classflex_gradient_operator_add0b4fb2eed25bc6386abd8219e516ff}} 
int {\bfseries index3\+Dto\+Linear} (int i, int j, int k)
\item 
\mbox{\Hypertarget{classflex_gradient_operator_a3dcfa503fb5b55b1c9629399bfb79c0d}\label{classflex_gradient_operator_a3dcfa503fb5b55b1c9629399bfb79c0d}} 
int {\bfseries index2\+Dto\+Linear} (int i, int j)
\item 
thrust\+::device\+\_\+vector$<$ T $>$ \hyperlink{classflex_gradient_operator_ac5cf151db87946ac0f1864344db260d5}{get\+Abs\+Row\+Sum\+C\+U\+DA} (bool transposed)
\begin{DoxyCompactList}\small\item\em same function as \hyperlink{classflex_gradient_operator_a04950a1e57f7587b95824bfd82b35738}{get\+Abs\+Row\+Sum()} but implemented in C\+U\+DA \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Additional Inherited Members}


\subsection{Detailed Description}
\subsubsection*{template$<$typename T$>$\newline
class flex\+Gradient\+Operator$<$ T $>$}

represents a gradient operator 

\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classflex_gradient_operator_aa83ae15b09ba2195951d8df803f08705}\label{classflex_gradient_operator_aa83ae15b09ba2195951d8df803f08705}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{flex\+Gradient\+Operator()}{flexGradientOperator()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
\hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::\hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator} (\begin{DoxyParamCaption}\item[{std\+::vector$<$ int $>$}]{A\+Input\+Dimension,  }\item[{int}]{a\+Grad\+Direction,  }\item[{\hyperlink{tools_8h_a40b3c158323c8bcb80e2095f3473213c}{gradient\+Type}}]{a\+Type,  }\item[{bool}]{a\+Minus }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



initializes the gradient operator 


\begin{DoxyParams}{Parameters}
{\em A\+Input\+Dimension} & vector of dimensions \\
\hline
{\em a\+Grad\+Direction} & direction of gradient. 0 for first dimension and so on. \\
\hline
{\em a\+Type} & type of gradient. Possible values are forward, backward and central. \\
\hline
{\em a\+Minus} & determines if operator is negated \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
\hyperlink{classflex_linear_operator_a7f986517e10aee21099ec7692b77905d}{is\+Minus} 
\end{DoxySeeAlso}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classflex_gradient_operator_a4b1480051ac7763da809c509685316d2}\label{classflex_gradient_operator_a4b1480051ac7763da809c509685316d2}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!copy@{copy}}
\index{copy@{copy}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{copy()}{copy()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
\hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$T$>$$\ast$ \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::copy (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



copies the linear operator 

\begin{DoxyReturn}{Returns}
copy of linear operator 
\end{DoxyReturn}


Implements \hyperlink{classflex_linear_operator_a7cc1425677cc30fcbd092ffd28d508c9}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_a04950a1e57f7587b95824bfd82b35738}\label{classflex_gradient_operator_a04950a1e57f7587b95824bfd82b35738}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!get\+Abs\+Row\+Sum@{get\+Abs\+Row\+Sum}}
\index{get\+Abs\+Row\+Sum@{get\+Abs\+Row\+Sum}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{get\+Abs\+Row\+Sum()}{getAbsRowSum()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
std\+::vector$<$T$>$ \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::get\+Abs\+Row\+Sum (\begin{DoxyParamCaption}\item[{bool}]{transposed }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



returns a vector of sum of absolute values per row used for preconditioning 


\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
vector of sum of absolute values per row 
\end{DoxyReturn}


Implements \hyperlink{classflex_linear_operator_ad6caa7b09e6e3c401cadef61b8e2307e}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_ac5cf151db87946ac0f1864344db260d5}\label{classflex_gradient_operator_ac5cf151db87946ac0f1864344db260d5}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!get\+Abs\+Row\+Sum\+C\+U\+DA@{get\+Abs\+Row\+Sum\+C\+U\+DA}}
\index{get\+Abs\+Row\+Sum\+C\+U\+DA@{get\+Abs\+Row\+Sum\+C\+U\+DA}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{get\+Abs\+Row\+Sum\+C\+U\+D\+A()}{getAbsRowSumCUDA()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
thrust\+::device\+\_\+vector$<$T$>$ \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::get\+Abs\+Row\+Sum\+C\+U\+DA (\begin{DoxyParamCaption}\item[{bool}]{transposed }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



same function as \hyperlink{classflex_gradient_operator_a04950a1e57f7587b95824bfd82b35738}{get\+Abs\+Row\+Sum()} but implemented in C\+U\+DA 


\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
vector of sum of absolute values per row 
\end{DoxyReturn}


Implements \hyperlink{classflex_linear_operator_a0a0a431d43f4f9d36cbee0d31ba5a29b}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_a6acb61ea8abf404d63be4574976391bb}\label{classflex_gradient_operator_a6acb61ea8abf404d63be4574976391bb}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!get\+Max\+Row\+Sum\+Abs@{get\+Max\+Row\+Sum\+Abs}}
\index{get\+Max\+Row\+Sum\+Abs@{get\+Max\+Row\+Sum\+Abs}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{get\+Max\+Row\+Sum\+Abs()}{getMaxRowSumAbs()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
T \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::get\+Max\+Row\+Sum\+Abs (\begin{DoxyParamCaption}\item[{bool}]{transposed }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



returns the maximum sum of absolute values per row used for preconditioning 


\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
maximum sum of absolute values per row 
\end{DoxyReturn}


Implements \hyperlink{classflex_linear_operator_afcb74697385ccb7c8d29870d7034c12a}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_aae5e807f99c3634c52b79f08d72fa7a2}\label{classflex_gradient_operator_aae5e807f99c3634c52b79f08d72fa7a2}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!times@{times}}
\index{times@{times}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{times()}{times()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
void \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::times (\begin{DoxyParamCaption}\item[{bool}]{transposed,  }\item[{const Tdata \&}]{input,  }\item[{Tdata \&}]{output }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



applies linear operator on vector 

equals $ y = Ax $ 
\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
{\em input} & data to be processed \\
\hline
{\em output} & output data \\
\hline
\end{DoxyParams}


Implements \hyperlink{classflex_linear_operator_a883982edf3be857815d2095e53f76e75}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_a287f5efd41aa14ee61aee87dfed08b88}\label{classflex_gradient_operator_a287f5efd41aa14ee61aee87dfed08b88}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!times\+Minus@{times\+Minus}}
\index{times\+Minus@{times\+Minus}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{times\+Minus()}{timesMinus()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
void \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::times\+Minus (\begin{DoxyParamCaption}\item[{bool}]{transposed,  }\item[{const Tdata \&}]{input,  }\item[{Tdata \&}]{output }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



applies linear operator on vector and substracts its result from y 

equals $ y = y - Ax $ 
\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
{\em input} & data to be processed \\
\hline
{\em output} & output data \\
\hline
\end{DoxyParams}


Implements \hyperlink{classflex_linear_operator_a62708874e134a649c8445df333079c69}{flex\+Linear\+Operator$<$ T $>$}.

\mbox{\Hypertarget{classflex_gradient_operator_a1b6c9b788e6d5a62ba008811f287f8e5}\label{classflex_gradient_operator_a1b6c9b788e6d5a62ba008811f287f8e5}} 
\index{flex\+Gradient\+Operator@{flex\+Gradient\+Operator}!times\+Plus@{times\+Plus}}
\index{times\+Plus@{times\+Plus}!flex\+Gradient\+Operator@{flex\+Gradient\+Operator}}
\subsubsection{\texorpdfstring{times\+Plus()}{timesPlus()}}
{\footnotesize\ttfamily template$<$typename T $>$ \\
void \hyperlink{classflex_gradient_operator}{flex\+Gradient\+Operator}$<$ T $>$\+::times\+Plus (\begin{DoxyParamCaption}\item[{bool}]{transposed,  }\item[{const Tdata \&}]{input,  }\item[{Tdata \&}]{output }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



applies linear operator on vector and adds its result to y 

equals $ y = y + Ax $ 
\begin{DoxyParams}{Parameters}
{\em transposed} & is true if operator should be (temporarily) transposed before usage \\
\hline
{\em input} & data to be processed \\
\hline
{\em output} & output data \\
\hline
\end{DoxyParams}


Implements \hyperlink{classflex_linear_operator_a3f2978ad1c5eae8cd4ae16deb2337416}{flex\+Linear\+Operator$<$ T $>$}.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
flex\+Gradient\+Operator.\+h\end{DoxyCompactItemize}
